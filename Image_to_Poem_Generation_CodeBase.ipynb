{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqB7dbc0k_RS"
      },
      "source": [
        "# **Image-to-Poem Generation Using Deep Generative Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBbhPvAk_RT"
      },
      "source": [
        "### Student Correspondences:\n",
        "\n",
        "1. Neha Anusooya Thimmarayi - nanusooy@depaul.edu\n",
        "2. Rohan Shankar Patil - rpatil5@depaul.edu\n",
        "\n",
        "#### Project Description:\n",
        "\n",
        "This project explores the use of deep generative models to generate creative, emotionally resonant poetry from visual inputs. The core objective is to develop a machine learning system that accepts an image and generates a corresponding poem that captures the image’s mood, theme, or aesthetic, rather than providing literal descriptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRzmc-MHk_RU"
      },
      "source": [
        "***Sufficient explanations on why each step is essential.\n",
        "Instructions on how to test each function with example cases to illustrate functionality.\n",
        "Commentary on the purpose of each implementation choice, especially if choices deviate from typical practices.***\n",
        "\n",
        "***(We'll remove all content which are in Italics)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8PH5YIbk_RV"
      },
      "source": [
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Introduction to Libraries\n",
        "\n",
        "This project utilizes a range of libraries for data processing, deep learning, and image-text modeling:\n",
        "\n",
        "1. **NumPy** and **Pandas** are used for efficient numerical operations and structured data manipulation.\n",
        "2. **PyTorch** is the core deep learning framework used to build and train models.\n",
        "3. **Torchvision** provides pre-trained models and image transformation utilities.\n",
        "4. **Transformers** (by Hugging Face) loads the pre-trained CLIP model for visual feature extraction.\n",
        "5. **Pillow (PIL)** and **Requests** help load and process image data from URLs.\n",
        "6. **tqdm** is used for progress bars during image processing and dataset iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.1 Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "iay8qID3k_RV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torchvision.models import resnet18\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is not available\n"
          ]
        }
      ],
      "source": [
        "# Check if a GPU is available and print the result (useful for performance monitoring)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMoQLJhLk_Ra"
      },
      "source": [
        "### 2. Model Design and Implementation\n",
        "\n",
        "This section describes the overall model pipeline, including how input data is preprocessed, how image features are extracted and projected into a textual embedding space, and how the resulting representations are used for poem generation using GPT-2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.1 Data Preprocessing and Dataset Preparation\n",
        "\n",
        "We begin by filtering and cleaning a multimodal dataset consisting of images and their corresponding poems. The dataset is parsed from JSON, cleaned to remove empty entries, and saved in structured CSV and JSON formats. Images are downloaded using their URLs, and only those with valid image-poem pairs are retained. A total of 899 usable pairs were selected for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC4C2skSk_RW",
        "outputId": "dac18e81-1bc0-48c1-8da4-c244a3802c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total entries in dataset: 8292\n",
            "Example entry:\n",
            "{\n",
            "  \"poem\": \"what is lovely never dies\\nbut passes into other loveliness\\nstar-dust or sea-foam flower or winged air\",\n",
            "  \"image_url\": \"https://farm2.staticflickr.com/1086/1002051357_0e9162423e.jpg\",\n",
            "  \"id\": 0\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Load the raw multimodal poem dataset from JSON and inspect its structure\n",
        "dataset_path = '/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/multim_poem.json'\n",
        "\n",
        "with open(dataset_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"Total entries in dataset: {len(data)}\")\n",
        "\n",
        "# Display a sample entry to understand the data format\n",
        "print(\"Example entry:\")\n",
        "print(json.dumps(data[0], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRAXzNPlk_RY",
        "outputId": "8a6a4c7c-9b3a-47e6-a34d-2cd38ebc7771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset size: 8292 entries\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_url</th>\n",
              "      <th>poem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://farm2.staticflickr.com/1086/1002051357...</td>\n",
              "      <td>what is lovely never dies\\nbut passes into oth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://farm8.staticflickr.com/7434/1002469112...</td>\n",
              "      <td>sods on the dugout begin to be fledged\\nwith f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://farm1.staticflickr.com/19/100255672_97...</td>\n",
              "      <td>one must have the mind of winter\\nto regard th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://farm2.staticflickr.com/1034/1002997433...</td>\n",
              "      <td>to put meaning in one's life may end in madnes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://farm4.staticflickr.com/3741/1004000893...</td>\n",
              "      <td>of living pained branches\\nmy garden's braided...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           image_url  \\\n",
              "0  https://farm2.staticflickr.com/1086/1002051357...   \n",
              "1  https://farm8.staticflickr.com/7434/1002469112...   \n",
              "2  https://farm1.staticflickr.com/19/100255672_97...   \n",
              "3  https://farm2.staticflickr.com/1034/1002997433...   \n",
              "4  https://farm4.staticflickr.com/3741/1004000893...   \n",
              "\n",
              "                                                poem  \n",
              "0  what is lovely never dies\\nbut passes into oth...  \n",
              "1  sods on the dugout begin to be fledged\\nwith f...  \n",
              "2  one must have the mind of winter\\nto regard th...  \n",
              "3  to put meaning in one's life may end in madnes...  \n",
              "4  of living pained branches\\nmy garden's braided...  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Filter out entries with missing or empty poems or image URLs\n",
        "cleaned_data = []\n",
        "\n",
        "for item in data:\n",
        "    poem = item.get(\"poem\")\n",
        "    url = item.get(\"img\") or item.get(\"image_url\")\n",
        "\n",
        "    if poem and url and poem.strip():\n",
        "        cleaned_data.append({\n",
        "            \"image_url\": url,\n",
        "            \"poem\": poem.strip()\n",
        "        })\n",
        "\n",
        "print(f\"Cleaned dataset size: {len(cleaned_data)} entries\")\n",
        "\n",
        "# Convert the cleaned data to a DataFrame for further processing\n",
        "df = pd.DataFrame(cleaned_data)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMX9Ra2ok_RY",
        "outputId": "bd9995f7-a16e-4d77-eec4-19481beb04a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset saved.\n"
          ]
        }
      ],
      "source": [
        "# Save the cleaned dataset in both CSV and JSON formats for future use\n",
        "os.makedirs(\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/processed\", exist_ok=True)\n",
        "df.to_csv(\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/processed/cleaned_poems.csv\", index=False)\n",
        "df.to_json(\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/processed/cleaned_poems.json\", orient=\"records\", indent=2)\n",
        "\n",
        "print(\"Cleaned dataset saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzKtPnv5k_RZ"
      },
      "outputs": [],
      "source": [
        "# Create a directory to store downloaded images (if it doesn't already exist)\n",
        "image_dir = \"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/images\"\n",
        "os.makedirs(image_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUFiUNsVk_RZ",
        "outputId": "66e33ec2-66e8-477e-bebb-5d0ae2345e5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:22<00:00, 44.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully downloaded: 899 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Helper function to download and save an image from a URL\n",
        "def download_image(url, save_path):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        response.raise_for_status()\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        img.save(save_path)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "# Sample 1000 image-poem pairs for downloading\n",
        "sample_df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "success_count = 0\n",
        "\n",
        "valid_data = []\n",
        "image_dir = \"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/images\"\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "img_id = 0  # counter for naming valid image files\n",
        "\n",
        "# Attempt to download each image; store successful image-poem pairs\n",
        "for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
        "    url = row['image_url']\n",
        "    poem = row['poem']\n",
        "    filename = f\"{img_id}.jpg\"\n",
        "    img_path = os.path.join(image_dir, filename)\n",
        "\n",
        "    if download_image(url, img_path):\n",
        "        valid_data.append({\n",
        "            \"image_filename\": filename,\n",
        "            \"poem\": poem\n",
        "        })\n",
        "        img_id += 1\n",
        "\n",
        "print(f\"Successfully downloaded: {len(valid_data)} images\")\n",
        "\n",
        "# Save the final list of valid image-poem pairs to CSV\n",
        "valid_df = pd.DataFrame(valid_data)\n",
        "valid_df.to_csv(\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/processed/filtered_poem_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnwNwQAuk_RZ",
        "outputId": "af68c93a-abb0-48c8-a3b9-00204c5fb01c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final usable pairs: 899\n"
          ]
        }
      ],
      "source": [
        "# Align image files with their corresponding poem entries by checking if the image exists\n",
        "valid_indices = [i for i in range(1000) if os.path.exists(f\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/images/{i}.jpg\")]\n",
        "\n",
        "# Create a filtered DataFrame of valid image-poem pairs\n",
        "filtered_df = sample_df.loc[valid_indices].reset_index(drop=True)\n",
        "print(f\"Final usable pairs: {len(filtered_df)}\")\n",
        "\n",
        "# Save the aligned dataset to CSV\n",
        "filtered_df.to_csv(\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/processed/filtered_poem_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2 Designing the Image Encoder\n",
        "\n",
        "To explore how different visual representations affect text generation, we implemented two image encoders:\n",
        "\n",
        "- **CLIP** (Contrastive Language–Image Pre-training): A vision-language model pre-trained on 400 million image-text pairs. We used the vision tower of the `openai/clip-vit-base-patch32` variant to extract 512-dimensional image embeddings. These embeddings are inherently normalized by the model.\n",
        "\n",
        "- **ResNet18**: A convolutional neural network pre-trained on ImageNet, used to extract 512-dimensional embeddings from images. We removed its final classification layer and normalized the output feature vectors.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.1  Designing the Image Encoder (CLIP - Contrastive Language-Image Pre-training)\n",
        "\n",
        "We used the `openai/clip-vit-base-patch32` model from Hugging Face Transformers to extract image embeddings. Each image is passed through the CLIP image processor to perform resizing, normalization, and tensor conversion. The processed image is then fed into the model's vision tower to obtain a 512-dimensional embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mNwQZLdWk_Ra"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load pre-trained CLIP model and processor\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Ensure the model is on the same device as your PyTorch setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "jsl0o0hWk_Ra"
      },
      "outputs": [],
      "source": [
        "def extract_image_features(image_path):\n",
        "    \"\"\"\n",
        "    Extract visual features from an image using CLIP.\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: Feature vector of shape (1, 512) representing the image.\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Extract features from the model\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.get_image_features(**inputs)\n",
        "    \n",
        "    # Normalize features (CLIP features are typically L2-normalized)\n",
        "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "    return image_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "oWCXvp25k_Ra"
      },
      "outputs": [],
      "source": [
        "# Test the feature extraction function\n",
        "def test_image_feature_extraction():\n",
        "    # Use a sample image from your dataset (e.g., the first valid image)\n",
        "    sample_image_path = \"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/images/0.jpg\"\n",
        "    if os.path.exists(sample_image_path):\n",
        "        features = extract_image_features(sample_image_path)\n",
        "        print(f\"Extracted feature shape: {features.shape}\")  # Expected: torch.Size([1, 512])\n",
        "        print(f\"Feature norm: {features.norm(dim=-1).item():.4f}\")  # Should be close to 1.0 due to normalization\n",
        "    else:\n",
        "        print(\"Sample image not found. Please ensure images are downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted feature shape: torch.Size([1, 512])\n",
            "Feature norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Run the test\n",
        "test_image_feature_extraction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features for the dataset (optional: process a subset for testing)\n",
        "def process_dataset_features(df, image_dir, output_path=\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/Feature Extraction/image_features.pt\"):\n",
        "    features_list = []\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        img_path = os.path.join(image_dir, row[\"image_filename\"])\n",
        "        if os.path.exists(img_path):\n",
        "            features = extract_image_features(img_path)\n",
        "            features_list.append(features.cpu())  # Move to CPU to save GPU memory\n",
        "    torch.save(torch.cat(features_list, dim=0), output_path)\n",
        "    print(f\"Saved {len(features_list)} image features to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:01<00:00,  5.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 10 image features to /workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/Feature Extraction/image_features.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Ensure valid_df is defined by reloading or recreating it if necessary\n",
        "if 'valid_df' not in globals():\n",
        "    # Load the filtered dataset if it exists, or recreate it\n",
        "    filtered_csv_path = \"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/processed/filtered_poem_data.csv\"\n",
        "    if os.path.exists(filtered_csv_path):\n",
        "        valid_df = pd.read_csv(filtered_csv_path)\n",
        "        print(f\"Loaded valid_df from {filtered_csv_path} with {len(valid_df)} entries\")\n",
        "    else:\n",
        "        print(\"Filtered dataset not found. Recreating from raw data...\")\n",
        "        valid_indices = [i for i in range(1000) if os.path.exists(f\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/images/{i}.jpg\")]\n",
        "        sample_df = pd.DataFrame(data).sample(n=1000, random_state=42).reset_index(drop=True)\n",
        "        valid_df = sample_df.loc[valid_indices].reset_index(drop=True)\n",
        "        valid_df.to_csv(filtered_csv_path, index=False)\n",
        "        print(f\"Recreated valid_df with {len(valid_df)} entries and saved to {filtered_csv_path}\")\n",
        "\n",
        "# Process a subset of the dataset (e.g., first 10 images) for testing\n",
        "subset_df = valid_df.head(10)\n",
        "process_dataset_features(subset_df, \"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 512])\n"
          ]
        }
      ],
      "source": [
        "loaded_features = torch.load(\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/Feature Extraction/image_features.pt\")\n",
        "print(loaded_features.shape)  # Should be torch.Size([10, 512])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.2 Designing the Image Encoder (ResNet18 - Convolutional Visual Features)\n",
        "\n",
        "ResNet18 was loaded using torchvision.models, and its final classification layer was removed. Images were preprocessed to match the model’s expected input format (224×224 resolution, normalized using ImageNet mean and std), and the 512-d feature vector was extracted from the penultimate layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "#Load ResNet18 model without final classification layer\n",
        "resnet_model = resnet18(pretrained=True)\n",
        "resnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1])\n",
        "resnet_model.eval().to(device)\n",
        "\n",
        "#Define standard image preprocessing steps for ResNet input\n",
        "resnet_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "]) \n",
        "\n",
        "#Extract and return a single 512-d ResNet feature vector from a given image\n",
        "def extract_resnet_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = resnet_transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        features = resnet_model(image).squeeze()\n",
        "    return features.cpu()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample feature shape: torch.Size([512])\n",
            "Norm before normalization: 31.320510864257812\n",
            "First 10 values: tensor([1.8201, 0.4422, 1.9081, 3.1979, 1.6765, 0.2751, 0.2356, 1.7061, 2.0732,\n",
            "        0.5250])\n"
          ]
        }
      ],
      "source": [
        "# Test the first ResNet feature before batch processing\n",
        "sample_path = os.path.join(image_dir, valid_df.loc[0, \"image_filename\"])\n",
        "sample_feature = extract_resnet_features(sample_path)\n",
        "\n",
        "print(\"Sample feature shape:\", sample_feature.shape)\n",
        "print(\"Norm before normalization:\", torch.norm(sample_feature).item())\n",
        "print(\"First 10 values:\", sample_feature[:10])\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Extract and normalize ResNet features for all images in the dataset\n",
        "def process_resnet_dataset(df, image_dir, output_path):\n",
        "    features_list = []\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        img_path = os.path.join(image_dir, row[\"image_filename\"])\n",
        "        if os.path.exists(img_path):\n",
        "            features = extract_resnet_features(img_path)\n",
        "            features = features / features.norm()  # L2-normalize\n",
        "            features_list.append(features.cpu())\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    \n",
        "    torch.save(torch.stack(features_list), output_path)\n",
        "    print(f\"Saved {len(features_list)} normalized ResNet features to {output_path}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 899/899 [00:58<00:00, 15.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 899 normalized ResNet features to /workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/features/resnet_features.pt\n"
          ]
        }
      ],
      "source": [
        "# Process the full dataset and save all normalized ResNet18 features to disk\n",
        "resnet_output_path = \"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/features/resnet_features.pt\"\n",
        "process_resnet_dataset(valid_df, image_dir, resnet_output_path)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ResNet features shape: torch.Size([899, 512])\n",
            "Norm sample: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Load the saved ResNet feature vectors and verify their shape and normalization\n",
        "resnet_output_path = \"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/features/resnet_features.pt\"\n",
        "resnet_features = torch.load(resnet_output_path)\n",
        "print(f\"Loaded ResNet features shape: {resnet_features.shape}\")\n",
        "print(\"Norm sample:\", resnet_features[0].norm().item()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3 Building the Dense Projection Network\n",
        "\n",
        "To bridge the gap between the 512-d image features and GPT-2’s 768-d input space, we use a shared projection network for both encoders. This network transforms the single 512-d vector into a sequence of 10 GPT-2-compatible prefix tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.1 Projection for CLIP Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.2 Projection for ResNet18 Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a projection network to map 512-d image features to a sequence of GPT-2-compatible prefix embeddings\n",
        "class ImageToTextProjection(nn.Module):\n",
        "    def __init__(self, input_dim=512, gpt2_emb_dim=768, prefix_length=10):\n",
        "        super(ImageToTextProjection, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt2_emb_dim = gpt2_emb_dim\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(input_dim, gpt2_emb_dim * prefix_length),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.projector(x)\n",
        "        return out.view(-1, self.prefix_length, self.gpt2_emb_dim)\n",
        "\n",
        "# Initialize the projection model\n",
        "projector = ImageToTextProjection(input_dim=512, gpt2_emb_dim=768, prefix_length=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projected shape: torch.Size([899, 10, 768])\n"
          ]
        }
      ],
      "source": [
        "# Project all ResNet features into GPT-2 embedding space as prefix tokens\n",
        "projected_all = projector(resnet_features)\n",
        "print(\"Projected shape:\", projected_all.shape) #Expected: (899, 10, 768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projected prefixes saved.\n"
          ]
        }
      ],
      "source": [
        "# Save the projected prefix embeddings to disk for use in GPT-2 conditioning\n",
        "torch.save(projected_all, \"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/features/resnet_projected_prefix.pt\")\n",
        "print(\"Projected prefixes saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 768])\n"
          ]
        }
      ],
      "source": [
        "# Load the filtered dataset and iterate over all image-poem pairs\n",
        "valid_df = pd.read_csv(\"/workspaces/Image-to-Poem-Generation-Using-Deep-Generative-Models/data/processed/filtered_poem_data.csv\")\n",
        "assert len(valid_df) == 899  \n",
        "\n",
        "# Iterate through image-poem pairs: for each projected prefix, retrieve the corresponding poem text\n",
        "for i in range(len(valid_df)):\n",
        "    image_prefix = projected_all[i]  # (10, 768)\n",
        "    poem = valid_df.loc[i, \"poem\"]\n",
        "\n",
        "print(image_prefix.shape)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.4 Configuring the GPT-2 Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.5 Model Validation on Dummy Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfbcU4XIk_Rb"
      },
      "source": [
        "### 3. Training Process\n",
        "\n",
        "*Outline your training pipeline, including data loading, pre-processing, and any regularization techniques.\n",
        "Briefly describe hyperparameters used (learning rate, batch size, epochs) and reasoning behind their choice.\n",
        "Include sample output or logs from training to illustrate model performance and learning curves.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1 Setting Up Training Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IG2PlbDk_Rb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW1aX5lVk_Rb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiA10bI1k_Rb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2 Training on a Small Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.3 Full Dataset Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.4 Visualizing Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxz5Rflkk_Rb"
      },
      "source": [
        "### 4. Evaluation Results\n",
        "\n",
        "*Present evaluation metrics and explain the criteria used to assess the model’s performance.\n",
        "Show example predictions or outputs to demonstrate model accuracy and behavior.\n",
        "Provide insights into the model’s strengths, weaknesses, and areas for improvement based on the results.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.1 Quantitative Evaluation with Automated Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a0R29f1k_Rb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFantz0Hk_Rc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnqBuv_0k_Rc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2 Qualitative Evaluation via Human Judgment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3 Error Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.4 Summary of Findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
